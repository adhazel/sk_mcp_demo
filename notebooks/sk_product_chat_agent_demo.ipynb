{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1394bf25",
   "metadata": {},
   "source": [
    "# Semantic Kernel MCP Orchestrator Demo\n",
    "\n",
    "This notebook demonstrates how to use the Semantic Kernel orchestrator to call the MCP RAG server.\n",
    "\n",
    "## Prerequisites\n",
    "- Make sure your MCP server is running on http://127.0.0.1:8002\n",
    "- Configure your Azure OpenAI or OpenAI credentials in .env\n",
    "- Install all dependencies using `poetry install`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d7ce8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1893847133.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mgraph TD\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import mermaid\n",
    "\n",
    "graph TD\n",
    "    A[React Frontend] --> B[FastAPI Server :8000]\n",
    "    B --> C[ProductChatAgent]\n",
    "    C --> D[ChatCompletionAgent]\n",
    "    D --> E[MCPStreamableHttpPlugin]\n",
    "    E --> F[MCP RAG Server :8002]\n",
    "    F --> G[ChromaDB Vector Search]\n",
    "    F --> H[SerpAPI Web Search]\n",
    "    F --> I[OpenAI GPT-4 for RAG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42702c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\aprilhazel\\Source\\sk_mcp_demo\\notebooks\n",
      "Python path: ['C:\\\\Users\\\\aprilhazel\\\\Source\\\\sk_mcp_demo\\\\notebooks', 'C:\\\\Users\\\\aprilhazel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip', 'C:\\\\Users\\\\aprilhazel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs']...\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = Path('.').resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path: {sys.path[:3]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ca7b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.utils.config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import our modules\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msk_product_chat_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProductChatAgent\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Modules imported successfully\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src.utils.config'"
     ]
    }
   ],
   "source": [
    "# Import our modules\n",
    "from src.utils.config import Config\n",
    "from src.agents.sk_product_chat_agent import ProductChatAgent\n",
    "\n",
    "print(\"‚úÖ Modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f77b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration and orchestrator\n",
    "config = Config()\n",
    "orchestrator = ProductChatAgent(config)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Environment: {config.environment}\")\n",
    "print(f\"  MCP Server URL: {config.mcp_server_url}\")\n",
    "print(f\"  AI Service: {config.openai_api_type}\")\n",
    "print(f\"  Temperature: {config.sk_temperature}\")\n",
    "print(\"\\n‚úÖ Orchestrator initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c4099",
   "metadata": {},
   "source": [
    "## 1. Health Check\n",
    "First, let's verify that everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274db95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check orchestrator status\n",
    "status = orchestrator.get_status()\n",
    "print(\"üìä Orchestrator Status:\")\n",
    "print(json.dumps(status, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844916b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MCP server health\n",
    "health_result = await orchestrator.mcp_plugin.health_check()\n",
    "print(f\"üè• MCP Server Health: {health_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b94401",
   "metadata": {},
   "source": [
    "## 2. Simple Product Search\n",
    "Let's search for products in the internal vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for laptop computers\n",
    "search_query = \"laptop computers\"\n",
    "search_result = await orchestrator.simple_search(search_query, limit=5)\n",
    "\n",
    "print(f\"üîç Search Results for '{search_query}':\")\n",
    "print(json.dumps(search_result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430abbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for wireless headphones\n",
    "search_query = \"wireless headphones\"\n",
    "search_result = await orchestrator.simple_search(search_query, limit=3)\n",
    "\n",
    "print(f\"üîç Search Results for '{search_query}':\")\n",
    "if \"error\" in search_result:\n",
    "    print(f\"‚ùå Error: {search_result['error']}\")\n",
    "else:\n",
    "    results = search_result.get('results', [])\n",
    "    print(f\"Found {len(results)} results:\")\n",
    "    for i, item in enumerate(results, 1):\n",
    "        name = item.get('name', 'Unknown')\n",
    "        score = item.get('similarity_score', 'N/A')\n",
    "        print(f\"  {i}. {name} (Score: {score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372a4a0a",
   "metadata": {},
   "source": [
    "## 3. Simple Chat Response\n",
    "Generate chat responses using the MCP server without full orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97211a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chat about laptops\n",
    "question = \"What laptops do you have available for gaming?\"\n",
    "chat_result = await orchestrator.simple_chat(\n",
    "    question=question,\n",
    "    use_web_search=True,\n",
    "    use_evaluation=False\n",
    ")\n",
    "\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"error\" in chat_result:\n",
    "    print(f\"‚ùå Error: {chat_result['error']}\")\n",
    "else:\n",
    "    response = chat_result.get('response', 'No response')\n",
    "    print(f\"ü§ñ Response: {response}\")\n",
    "    \n",
    "    # Show sources if available\n",
    "    sources = chat_result.get('sources', {})\n",
    "    if sources:\n",
    "        internal_count = len(sources.get('internal', []))\n",
    "        web_count = len(sources.get('web', []))\n",
    "        print(f\"\\nüìö Sources: {internal_count} internal, {web_count} web\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91318f25",
   "metadata": {},
   "source": [
    "## 4. Evaluated Chat Response\n",
    "Generate chat responses with evaluation and risk scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluated chat with risk scoring\n",
    "question = \"What are the best gaming laptops with RTX 4080 graphics cards?\"\n",
    "eval_result = await orchestrator.simple_chat(\n",
    "    question=question,\n",
    "    use_web_search=True,\n",
    "    use_evaluation=True\n",
    ")\n",
    "\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"error\" in eval_result:\n",
    "    print(f\"‚ùå Error: {eval_result['error']}\")\n",
    "else:\n",
    "    response = eval_result.get('response', 'No response')\n",
    "    print(f\"ü§ñ Response: {response}\")\n",
    "    \n",
    "    # Show evaluation metrics if available\n",
    "    evaluation = eval_result.get('evaluation', {})\n",
    "    if evaluation:\n",
    "        print(\"\\nüìä Evaluation Metrics:\")\n",
    "        print(f\"  ‚Ä¢ Confidence Score: {evaluation.get('confidence_score', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ Risk Score: {evaluation.get('risk_score', 'N/A')}\")\n",
    "        print(f\"  ‚Ä¢ Hallucination Risk: {evaluation.get('hallucination_risk', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4cbc7",
   "metadata": {},
   "source": [
    "## 5. Full Orchestrated Response\n",
    "Use the Semantic Kernel planner for intelligent orchestration of multiple MCP tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cdc5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full orchestrated response\n",
    "question = \"I need a powerful laptop for machine learning work. What do you recommend?\"\n",
    "context = \"I'm a data scientist and need something with good GPU support and lots of RAM\"\n",
    "\n",
    "orchestrated_result = await orchestrator.process_question(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    use_evaluation=True\n",
    ")\n",
    "\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(f\"üìù Context: {context}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if orchestrated_result.get(\"status\") == \"failed\":\n",
    "    print(f\"‚ùå Error: {orchestrated_result.get('error', 'Unknown error')}\")\n",
    "    if orchestrated_result.get('details'):\n",
    "        print(f\"Details: {orchestrated_result['details']}\")\n",
    "else:\n",
    "    response = orchestrated_result.get('response', {})\n",
    "    if isinstance(response, dict):\n",
    "        main_response = response.get('response', str(response))\n",
    "    else:\n",
    "        main_response = str(response)\n",
    "    \n",
    "    print(f\"üéº Orchestrated Response: {main_response}\")\n",
    "    print(f\"\\nüìä Status: {orchestrated_result.get('status', 'unknown')}\")\n",
    "    print(f\"ü§ñ Orchestrator: {orchestrated_result.get('orchestrator', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694b94c",
   "metadata": {},
   "source": [
    "## 6. Available Functions\n",
    "Let's see what functions are available in the orchestrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get available functions\n",
    "functions = await orchestrator.get_available_functions()\n",
    "print(\"üîß Available Functions:\")\n",
    "print(json.dumps(functions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a9801",
   "metadata": {},
   "source": [
    "## 7. Interactive Testing\n",
    "Create a simple interactive interface to test different questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different types of questions\n",
    "test_questions = [\n",
    "    \"What gaming keyboards do you have?\",\n",
    "    \"Tell me about the latest AI developments in 2024\",\n",
    "    \"What's the best laptop for video editing?\",\n",
    "    \"Do you have any wireless mice with good battery life?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Test Question {i}: {question}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Use orchestrated response for comprehensive answers\n",
    "    result = await orchestrator.process_question(\n",
    "        question=question,\n",
    "        use_evaluation=True\n",
    "    )\n",
    "    \n",
    "    if result.get(\"status\") == \"failed\":\n",
    "        print(f\"‚ùå Error: {result.get('error', 'Unknown error')}\")\n",
    "    else:\n",
    "        response = result.get('response', {})\n",
    "        if isinstance(response, dict):\n",
    "            main_response = response.get('response', str(response))\n",
    "        else:\n",
    "            main_response = str(response)\n",
    "        \n",
    "        print(f\"ü§ñ Response: {main_response}\")\n",
    "        print(f\"Status: {result.get('status', 'unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2bdc5",
   "metadata": {},
   "source": [
    "## 8. Performance and Diagnostics\n",
    "Check the performance and status of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f901e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status check\n",
    "print(\"üèÅ Final System Status:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Orchestrator status\n",
    "status = orchestrator.get_status()\n",
    "print(\"üìä Orchestrator:\")\n",
    "for key, value in status.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# MCP server health\n",
    "health = await orchestrator.mcp_plugin.health_check()\n",
    "print(f\"\\nüè• MCP Server: {health}\")\n",
    "\n",
    "print(\"\\n‚úÖ Demo completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sk-mcp-demo-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
